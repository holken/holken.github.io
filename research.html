---
layout: layout
title: "Research Overview"
---

<center>  
  Here's my <a href="https://scholar.google.com/citations?user=etBgLN4AAAAJ&hl=en">Google Scholar</a>. <br>
  Jump to <a href='#publications'> selected publications</a>.
</center>

<div class="divider"></div>

<h2 id='publications' class="page-heading"> Selected Publications</h1>

<div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/iros23.png">
    <table>
      <tr>          
        <td><a href="https://www.diva-portal.org/smash/get/diva2:1787953/FULLTEXT01.pdf">DiVA</a></td>
      </tr>
    </table>
  </div>

  <div class="six columns">

    <b> VARIQuery: VAE Segment-based Active Learning for Query Selection in Preference-based Reinforcement Learning </b>
    <p> Daniel Marta*, <b>Simon Holk*</b>, Christian Pek, Jana Tumova, and Iolanda Leite <br />
    IROS'23 </p>
  <p> This paper addresses the
    often-overlooked aspect of query selection, which is closely
    related to active learning (AL). We propose a novel query
    selection approach that leverages variational autoencoder (VAE)
    representations of state sequences. In this manner, we formulate
    queries that are diverse in nature while simultaneously taking
    into account reward model estimations.
  </p>
  </div>
</div>

<div class="divider"></div>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/icra23.png">
      <table>
        <tr>          
          <td><a href="https://www.diva-portal.org/smash/get/diva2:1744884/FULLTEXT01.pdf">DiVA</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Aligning Human Preferences with Baseline Objectives in Reinforcement Learning </b>
      <p> Daniel Marta, <b>Simon Holk</b>, Christian Pek, Jana Tumova, and Iolanda Leite <br />
      ICRA'23 </p>
    <p> By considering baseline objectives to be designed
      beforehand, we are able to narrow down the policy space, solely
      requesting human attention when their input matters the most.
      To allow for control over the optimization of different objectives,
      our approach contemplates a multi-objective setting. We achieve
      human-compliant policies by sequentially training an optimal
      policy from a baseline specification and collecting queries on
      pairs of trajectories. 
    </p>
    </div>
  </div>

</div>
